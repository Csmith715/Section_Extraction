{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import difflib\n",
    "from itertools import combinations, chain, groupby\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Previously Generated Rule Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('qterms.data', 'rb') as filehandle:\n",
    "    qterms = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('whatrules2.data', 'rb') as filehandle:\n",
    "    what_rules2 = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Creation Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, independent work had been done to manually extract the sections of text on a webinar page that define \"What the webinar is about\" or \"What you will learn\".  Additionally, a pre-defined set of search queue terms was developed to use as a marker that would help differntiate this section from the rest of the text.\n",
    "\n",
    "The biggest challenge faced so far is that while the POS tags and rules can be created as described below, it may be missing text some of the time or it may be returning too much as it over generalizes certain sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Search the Researcher outputs for exact string matches to the existing queues. If found, return the rule.**\n",
    "\n",
    "### For Example:\n",
    "\n",
    "Original Text:\n",
    "\n",
    "**Join us to learn how to:**\n",
    "\n",
    "- **Create a multi-touch promotion campaign that includes email and other paid promotions**\n",
    "- **Design and execute a focused PR strategy that will get the attention of attendees and analysts**\n",
    "- **Develop other creative ways to get the attention of your customers and prospects**\n",
    "\n",
    "\n",
    "2. **The following reference queues would be extracted:**\n",
    "\n",
    "['Join us to learn',\n",
    " 'to learn',\n",
    " 'Join us to learn how to',\n",
    " 'Join us to learn how',\n",
    " 'Join us']\n",
    " \n",
    " \n",
    "3. **These would then be converted to the POS rules based on their original place in the text**\n",
    " \n",
    " [('VERB', 'VB', 'ROOT'),\n",
    "  ('PRON', 'PRP', 'dobj'),\n",
    "  ('PART', 'TO', 'aux'),\n",
    "  ('VERB', 'VB', 'xcomp')],\n",
    "  \n",
    " [('PART', 'TO', 'aux'), ('VERB', 'VB', 'xcomp')],\n",
    " \n",
    " [('VERB', 'VB', 'ROOT'),\n",
    "  ('PRON', 'PRP', 'dobj'),\n",
    "  ('PART', 'TO', 'aux'),\n",
    "  ('VERB', 'VB', 'xcomp'),\n",
    "  ('ADV', 'WRB', 'advmod'),\n",
    "  ('PART', 'TO', 'aux')],\n",
    "  \n",
    " [('VERB', 'VB', 'ROOT'),\n",
    "  ('PRON', 'PRP', 'dobj'),\n",
    "  ('PART', 'TO', 'aux'),\n",
    "  ('VERB', 'VB', 'xcomp'),\n",
    "  ('ADV', 'WRB', 'advmod')],\n",
    "  \n",
    " [('VERB', 'VB', 'ROOT'), ('PRON', 'PRP', 'dobj')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spacy Phrase Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more refined and specific approach we used Spacy's Phrase Matching utility. This did a great job of identifying more specific text. But did not provide much in the way of finding generalized patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qterms = [q for q in queWhat.key_queue if 1 < len(q.split(' ')) < 8]\n",
    "#new_terms = ['will walk you through', 'In this presentation', 'Attend this webinar and learn']\n",
    "#qterms.extend(new_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save qterms for spacy phrase matching\n",
    "#with open('qterms.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "#    pickle.dump(qterms, filehandle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "terms = qterms\n",
    "# Only run nlp.make_doc to speed things up\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", None, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the sentence(s) containing search matches\n",
    "\n",
    "def spacy_phrase_match(text):\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matching_sents = []\n",
    "    for match_id, start, end in matches:\n",
    "        sent_span = doc[start:end].sent\n",
    "        matching_sents.append(sent_span.text)\n",
    "    \n",
    "    return pd.unique(matching_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match rule patterns in text to POS rules\n",
    "def get_qphrases_text(rules, text):\n",
    "    fnd = []\n",
    "    pat = get_pattern_tupples(text)\n",
    "    for w in rules:\n",
    "        rm = rule_matches(pat, w)\n",
    "        if rm:\n",
    "            fnd.append(text)\n",
    "    \n",
    "    return(fnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Spacy Matcher to refine the output\n",
    "def extract_relevant_chunk(about_text_sentences):\n",
    "    ats = about_text_sentences.split('\\n')\n",
    "    refined_ats = [a for a in ats if spacy_phrase_match(a).size > 0]\n",
    "    \n",
    "    return '\\n'.join(refined_ats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_difference(a, b):\n",
    "    return list({*a} ^ {*b})\n",
    "\n",
    "def symmetric_same(a,b):\n",
    "    return list({*a} & {*b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the similarity of two bodies of text\n",
    "def find_sim(a, b):\n",
    "    nlpa = nlp(str(a))\n",
    "    nlpb = nlp(str(b))\n",
    "    return(nlpa.similarity(nlpb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to create POS tupples with Spacy\n",
    "def get_pattern_tupples(text):\n",
    "    doc = nlp(text)\n",
    "    pattern = [(t.pos_, t.tag_, t.dep_) for t in doc]\n",
    "    \n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(a, b):\n",
    "    sm = difflib.SequenceMatcher(None, a, b).get_matching_blocks()\n",
    "    seq_match = [tuple(a[s[0]:s[0]+s[2]]) for s in sm if s[2] > 1]\n",
    "    return(seq_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function used to determine if a sequence of POS tupples in a document match the \n",
    "# sequence of tupples in the rules\n",
    "def rule_matches(a, b):\n",
    "    sm = difflib.SequenceMatcher(None, a, b).get_matching_blocks()\n",
    "    seq_match = [[s[0],s[0]+s[2]] for s in sm if s[2] == len(b)]\n",
    "    return(seq_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract \"What someone will learn about\" in the text, if in listed form\n",
    "\n",
    "def extract_listed_about(text, rules):\n",
    "    ulist = text.split('\\n')\n",
    "    ulist = [u for u in ulist if u]\n",
    "    list_idxs = []\n",
    "    for i, x in enumerate(ulist):\n",
    "        if x[:2] == '- ':\n",
    "            list_idxs.append(i)\n",
    "    \n",
    "    list_groups = []\n",
    "    for k, g in groupby(enumerate(list_idxs), lambda x: x[0]-x[1]):\n",
    "         list_groups.append(list(map(itemgetter(1), g)))\n",
    "\n",
    "    webinar_list_text = []\n",
    "    new_list_idxs = []\n",
    "    for g in list_groups:\n",
    "        if g[0] != 0:\n",
    "            a = min(g)-1\n",
    "            list_text = '\\n'.join(ulist[a:max(g)+1])\n",
    "            list_idxs.append(a)\n",
    "        else:\n",
    "            list_text = ''\n",
    "        webinar_list_text.append(list_text)\n",
    "        new_list_idxs.append(g)  \n",
    "\n",
    "    wlt = [get_qphrases_text(rules, s) for s in webinar_list_text]\n",
    "    wlt = pd.unique(list(chain.from_iterable(wlt)))\n",
    "    spacy_lt = [spacy_phrase_match(s) for s in webinar_list_text]\n",
    "    spacy_lt = pd.unique(list(chain.from_iterable(spacy_lt)))\n",
    "    if wlt.size > 1:\n",
    "        list_max = np.argmax([find_sim(w, spacy_lt) for w in wlt])\n",
    "        wlt = wlt[list_max]\n",
    "    elif wlt.size != 0: \n",
    "        wlt = wlt[0]\n",
    "    return wlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract \"What someone will learn about\" in the text, if in sentence form\n",
    "\n",
    "def extract_sentence_about(text, listed_about, rules):\n",
    "    if type(listed_about) == str:\n",
    "        listed_about = listed_about.split('\\n')\n",
    "    ulist = text.split('\\n')\n",
    "    ulist = [u for u in ulist if u]\n",
    "    if len(listed_about) == 0:  # Listed 'You will learn' doesn't exist\n",
    "        nlt = [get_qphrases_text(rules, n) for n in ulist]\n",
    "        nlt = pd.unique(list(chain.from_iterable(nlt)))\n",
    "        sp_nlt = [a for a in nlt if spacy_phrase_match(a).size > 0]\n",
    "    else:  # Listed 'You will learn' does exist \n",
    "        ss = symmetric_same(listed_about, ulist)\n",
    "        sidxs = []\n",
    "        for s in ss:\n",
    "            sidxs.append(ulist.index(s))\n",
    "        ulist = [u for i, u in enumerate(ulist) if i < min(sidxs)]\n",
    "        nlt = [get_qphrases_text(rules, n) for n in ulist]\n",
    "        nlt = pd.unique(list(chain.from_iterable(nlt)))\n",
    "        sp_nlt = [a for a in nlt if spacy_phrase_match(a).size > 0]        \n",
    "    return sp_nlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all remaining text that does not match the \"What you will learn pattern\"\n",
    "\n",
    "def extract_introduction(text, listed_about, sentence_about):\n",
    "    if type(listed_about) == str:\n",
    "        listed_about = listed_about.split('\\n')\n",
    "    if type(sentence_about) == str:\n",
    "        sentence_about = sentence_about.split('\\n')\n",
    "    ulist = text.split('\\n')\n",
    "    ulist = [u for u in ulist if u]\n",
    "# Listed 'You will learn' does exist and Sentence does not\n",
    "    if len(listed_about) != 0 and len(sentence_about) == 0:  \n",
    "        ss = symmetric_same(listed_about, ulist)\n",
    "        sidxs = []\n",
    "        for s in ss:\n",
    "            sidxs.append(ulist.index(s))\n",
    "        ulist = [u for i, u in enumerate(ulist) if i < min(sidxs)]\n",
    "        intro = [x for x in ulist if x[0] != '#']\n",
    "        intro = [x for x in intro if x[:2] != '- ']\n",
    "# Listed 'You will learn' does not exist and Sentence does         \n",
    "    elif len(listed_about) == 0 and len(sentence_about) != 0: \n",
    "        ss = symmetric_same(sentence_about, ulist)\n",
    "        sidxs = []\n",
    "        for s in ss:\n",
    "            sidxs.append(ulist.index(s))\n",
    "        ulist = [u for i, u in enumerate(ulist) if i < min(sidxs)]\n",
    "        intro = [x for x in ulist if x[0] != '#'] \n",
    "        intro = [x for x in intro if x[:2] != '- ']\n",
    "# Both Listed and Sentence exist        \n",
    "    elif len(listed_about) != 0 and len(sentence_about) != 0: \n",
    "        ss = symmetric_same(sentence_about, ulist)\n",
    "        sidxs = []\n",
    "        for s in ss:\n",
    "            sidxs.append(ulist.index(s))\n",
    "        ulist = [u for i, u in enumerate(ulist) if i < min(sidxs)]\n",
    "        intro = [x for x in ulist if x[0] != '#']\n",
    "        intro = [x for x in intro if x[:2] != '- ']\n",
    "# Neither exist:\n",
    "    elif len(listed_about) == 0 and len(sentence_about) == 0:\n",
    "        intro = []\n",
    "    \n",
    "    return pd.unique(intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function written for our tentative interface\n",
    "\n",
    "def extract_sections(text):\n",
    "    ela = extract_listed_about(text, rules=what_rules2)\n",
    "    est = extract_sentence_about(text, ela, rules=what_rules2)\n",
    "    intro = extract_introduction(text, ela, est)\n",
    "    \n",
    "    l_list = ela.split('\\n')\n",
    "    split_text = text.split('\\n')\n",
    "\n",
    "    ela = split_text[split_text.index(l_list[0]):split_text.index(l_list[-1])+1] if len(l_list) != 0 else ''\n",
    "    intro = split_text[split_text.index(intro[0]):split_text.index(intro[-1])+1] if len(intro) != 0 else ''\n",
    "    est = split_text[split_text.index(est[0]):split_text.index(est[-1])+1] if len(est) != 0 else ''\n",
    "    \n",
    "    extract_output = {\n",
    "        'introduction': '\\n'.join(intro),\n",
    "        'learn_about_sentence': '\\n'.join(est),\n",
    "        'learn_about_list': '\\n'.join(ela)\n",
    "    }\n",
    "    \n",
    "    with open('cware_extract_output.json', 'w') as json_file:\n",
    "        json.dump(extract_output, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to create initial POS rules based on pre-defined text queues\n",
    "\n",
    "def create_rules(q_terms, text_to_search):\n",
    "    rules = []\n",
    "    pat, t = get_pattern_tupples(text_to_search)\n",
    "    for q in q_terms:\n",
    "        rm = rule_matches(t, q)\n",
    "        if rm:\n",
    "            rm = list(chain.from_iterable(rm))\n",
    "            rules.append(pat[rm[0]:rm[1]])\n",
    "    \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Process for Extracting Sections\n",
    "\n",
    "1. Split Text by New Line\n",
    "2. Identify the \"What You'll Learn\" Section\n",
    "3. Identify the Introduction Section (always above the \"What You'll Learn\" section)\n",
    "4. Identify the speaker information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_webinar_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Homepage\n",
      "- Uncategorized\n",
      "- Feb 18 Webinar: Data Governance Reality Check# Feb 18 Webinar: Data Governance Reality Check\n",
      "\n",
      "September 20, 2019\n",
      "\n",
      "#### DATEFebruary 18, 2020, This webinar has passed. The recording will be made available On Demand within the next two US business days.\n",
      "\n",
      "#### TIME: 2 PM Eastern / 11 AM Pacific\n",
      "\n",
      "#### PRICE: Free to all attendees\n",
      "\n",
      "# This webinar is sponsored by:\n",
      "\n",
      "## About the Webinar\n",
      "\n",
      "It’s been almost two years since the General Data Protection Regulation shook up how organizations manage data security and privacy, ushering in a new focus on Data Governance. This complex but critical practice still has most enterprises grappling to master it for a myriad of reasons.\n",
      "\n",
      "In this webinar, we’ll examine how Data Governance attitudes and practices continue to evolve and discuss what new research reveals as the most predominant challenges. We’ll delve into technology trends, including how adding certain capabilities will benefit your organization in terms of data asset availability, quality, and usability, including data consumer literacy and confidence.\n",
      "\n",
      "When you attend this webinar, you will learn about:\n",
      "\n",
      "- The requirements for a successful and sustainable Data Governance program\n",
      "- Increasing confidence in data analytics for faster speed to insights\n",
      "- How to automate data preparation and intelligence and where to startAll registrants will receive a copy of the new erwin white paper, The 2020 State of Data Governance and Automation, which is based on a recent survey conducted by erwin and DATAVERSITY.\n",
      "\n",
      "## About the Speaker\n",
      "\n",
      "Danny Sandwell\n",
      "\n",
      "Director of Product Marketing, erwin, Inc.\n",
      "\n",
      "Danny Sandwell is an IT industry veteran with more than 30 years of experience. As Director of Product Marketing for erwin, he is responsible for communicating the technical capabilities and business value of the company’s data modeling and data intelligence solutions. During Danny’s 20+ years with the company, he has also worked in pre-sales consulting, product management, business development, and business strategy roles – all giving him opportunities to engage with customers across various industries as they plan, develop, and manage their data architectures. His goal is to help enterprises unlock the value of their data assets to produce the desired results while mitigating data-related risks.\n"
     ]
    }
   ],
   "source": [
    "# Original Text\n",
    "t = 31\n",
    "print(df.useable_text[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "September 20, 2019 \n",
      "\n",
      " #### DATEFebruary 18, 2020, This webinar has passed. The recording will be made available On Demand within the next two US business days.\n",
      "# This webinar is sponsored by:\n",
      "In this webinar, we’ll examine how Data Governance attitudes and practices continue to evolve and discuss what new research reveals as the most predominant challenges. We’ll delve into technology trends, including how adding certain capabilities will benefit your organization in terms of data asset availability, quality, and usability, including data consumer literacy and confidence. \n",
      "\n",
      " When you attend this webinar, you will learn about:\n",
      "- The requirements for a successful and sustainable Data Governance program\n",
      "- Increasing confidence in data analytics for faster speed to insights\n",
      "- How to automate data preparation and intelligence and where to startAll registrants will receive a copy of the new erwin white paper, The 2020 State of Data Governance and Automation, which is based on a recent survey conducted by erwin and DATAVERSITY.\n"
     ]
    }
   ],
   "source": [
    "# Parsed Sections\n",
    "\n",
    "ela = extract_listed_about(df.useable_text[t], what_rules2)\n",
    "est = extract_sentence_about(df.useable_text[t], what_rules2, ela)\n",
    "intro = extract_introduction(df.useable_text[t], ela, est)\n",
    "#print(intro, est, '\\n\\n', ela)\n",
    "print('\\n'.join(intro), '\\n\\n', '\\n'.join(est), '\\n\\n', ela)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Creation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "queues = pd.read_csv('content_queues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "queWhat = queues[queues['type'] == 'What'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf = pd.read_csv('bart_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "utext = udf.useable_text\n",
    "udesc = [x for x in udf['$description'] if x != 'None']\n",
    "utext = [t.replace('\\n\\n\\n', '') for t in utext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_remove = ['View cookie settings', 'We use cookies to','Cookies Policy', 'This website uses cookies to']\n",
    "utext = [remove_unwanted_text(u, text_to_remove) for u in utext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_terms = []\n",
    "q_pats = []\n",
    "for q in queWhat.key_queue:\n",
    "    p, terms = get_pattern_tupples(q)\n",
    "    q_terms.append(terms)\n",
    "    q_pats.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_rules = []\n",
    "for fb_post in df.FB_Extract:\n",
    "    what_rules.extend(create_rules(q_terms, fb_post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rules = []\n",
    "for ud in udesc:\n",
    "    new_rules.extend(create_rules(q_terms, ud))\n",
    "what_rules.extend(new_rules)\n",
    "what_rules = list(np.unique(what_rules))\n",
    "\n",
    "# Rules that don't have single tokens\n",
    "what_rules2 = [r for r in what_rules if len(r) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save what rules\n",
    "with open('whatrules2.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(what_rules2, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
